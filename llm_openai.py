# -*- coding: utf-8 -*-
"""LLM_OPENAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f2RszF0wa1iG0jf5sQnViynBwyALoQ2c

#Document Load()
"""

pip install langchain

pip install unstructured libmagic python-magic python-magic-bin

pip install unstructured

pip install faiss-cpu

!pip install openai

pip install sentence-transformers

pip install tiktoken

import os
import pickle
import time
import langchain
from langchain import OpenAI
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import UnstructuredURLLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from sentence_transformers import SentenceTransformer

loaders = UnstructuredURLLoader(urls=[
    "https://www.who.int/initiatives/behealthy/healthy-diet",
    "https://www.nhs.uk/live-well/eat-well/how-to-eat-a-balanced-diet/eight-tips-for-healthy-eating/",
    "https://www.hsph.harvard.edu/nutritionsource/healthy-eating-plate/",
    "https://www.cdc.gov/healthyweight/healthy_eating/index.html",
    "https://environmental-conscience.com/causes-effects-solutions-for-public-health-issues/",
    "http://bbrc.in/bbrc/papers/pdf%20files/Volume%208%20-%20No%202%20-%202015/3.pdf",
    "https://en.wikipedia.org/wiki/Health",
    "https://www.medicalnewstoday.com/articles/150999",
    "https://en.wikipedia.org/wiki/Diet_(nutrition)"
])

# Now you can use the url_loader object to process the URLs

data = loaders.load()
len(data)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

# As data is of type documents we can directly use split_documents over split_text in order to get the chunks.
docs = text_splitter.split_documents(data)

len(docs)

docs[0]

len(docs)

os.environ['OPENAI_API_KEY'] = "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"
llm = OpenAI(temperature=0.9, max_tokens=500)

# Create the embeddings of the chunks using openAIEmbeddings
embeddings = OpenAIEmbeddings()

# Pass the documents and embeddings inorder to create FAISS vector index
vectorindex_openai = FAISS.from_documents(docs, embeddings)

chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorindex_openai.as_retriever())
chain

import pandas as pd

queries = input("Enter your Query")
# Perform the queries and collect responses
responses = {}
for query in queries:
    response = chain({"question": query}, return_only_outputs=True)
    responses[query] = response
    print(responses)

# Organize responses into a DataFrame
# df = pd.DataFrame(responses.items(), columns=['Query', 'Response'])

# # Save responses to CSV
# df.to_csv('canoo_info.csv', index=False)

